# ğŸ§  Prompt Evaluation Framework

Prompt Evaluation framework for evaluating and refining prompts using both **automated metrics** (BLEU, F1) and **LLM-assisted evaluation (ChatGPT)**. It helps identify weak prompts, generate improved versions, and visualize the results in a dashboard.

---

## ğŸ“ Project Structure

```
Prompt_Evaluation_Framework/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ prompts.yaml               # Input prompts with metadata and references
â”‚
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ results.csv                # Auto-generated evaluation results
â”‚   â””â”€â”€ refined_prompts.yaml       # Auto-generated improved prompts
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ llm_interface.py           # API call to LLM (OpenRouter / ChatGPT)
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ metrics.py                 # BLEU and F1 evaluation functions
â”‚   â””â”€â”€ chatgpt_assisted.py        # ChatGPT-based structured judgment + refinement
â”‚
â”œâ”€â”€ tuner/
â”‚   â””â”€â”€ prompt_tuner.py            # refine_prompt() logic with conditional refinement
â”‚
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py                     # Streamlit dashboard for exploring evaluation results
â”‚
â”œâ”€â”€ main.py                        # Main script for running prompt evaluation pipeline
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âœ… Features

- ğŸ” **Prompt Evaluation** using:
  - BLEU and F1 for automatic scoring
  - ChatGPT-assisted judgment (relevance, correctness, completeness, etc.)
- ğŸ› ï¸ **Prompt Refinement** only when:
  - Evaluation scores are poor
  - Response is vague or incomplete
- ğŸ“Š **Streamlit Dashboard** to explore:
  - Original & refined prompts
  - Model responses and reference answers
  - Evaluation scores & LLM feedback

---

## âš™ï¸ Installation

```bash
# Install dependencies
pip install -r requirements.txt
```

> âš ï¸ Requires Python 3.8+

---

## ğŸš€ Running the Evaluation

1. **Add prompts** in `data/prompts.yaml`:

```yaml
- prompt_id: QA_001
  text: What is the capital of France?
  intent: factual_qa
  strategy: direct
  complexity: low
  reference: Paris
```

2. **Run the main evaluation script**:

```bash
python main.py
```

This will:

- Generate model responses
- Evaluate using BLEU, F1, and ChatGPT
- Refine weak prompts and save results to:
  - `output/results.csv`
  - `output/refined_prompts.yaml`

---

## ğŸ“Š Launch the Dashboard

```bash
streamlit run dashboard/app.py
```

Features:

- Toggle and explore each prompt block
- View model responses vs reference
- See ChatGPT feedback and suggested improvements

---

## ğŸ”§ LLM Configuration

LLM API is accessed via `models/llm_interface.py` using OpenRouter or any supported LLM endpoint.

### Example using OpenRouter:

```python
ENDPOINT = "https://openrouter.ai/api/v1/chat/completions"
HEADERS = {
    "Authorization": f"Bearer {your_openrouter_api_key}",
    "Content-Type": "application/json"
}
```

## ğŸ§ª Key Components

| File                  | Description                                              |
| --------------------- | -------------------------------------------------------- |
| `main.py`             | full pipeline: LLM â†’ metrics â†’ ChatGPT eval â†’ refinement |
| `prompt_tuner.py`     | Smart logic to only refine weak or vague prompts         |
| `chatgpt_assisted.py` | Handles structured evaluation + refinement via ChatGPT   |
| `app.py`              | Interactive UI to analyze prompts, outputs, and feedback |

---

## ğŸ§  Example: Refinement Logic

A prompt like:

> `Summarize the article: The solar system consists of...`

Will trigger refinement if the model responds:

> `"Please provide the full article..."`

ğŸ”„ The framework will automatically improve it to:

> `Summarize this passage about the solar system: [insert text here]`

---

## ğŸ§¼ Cleanup Output

To start fresh:

```bash
rm output/results.csv
rm output/refined_prompts.yaml
```

---

## ğŸ“Œ Requirements

```txt
streamlit
pandas
pyyaml
requests
scikit-learn
openai
```

---

## ğŸ“¬ Contact

Feel free to reach out if you need help integrating this with your custom LLM backend or want to extend the refinement logic.
